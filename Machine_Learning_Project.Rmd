---
title: "Practical_Machine_Learning_Project"
author: "Md Ahmed"
date: "August 9th, 2017"
output:
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Project Report: Machine Learning Algorithms
We are given two sets of data collected from accelerometers placed on the belt, forearm, arm, and dumbell of 6 research study participants for this machine learning project. Training data stems from accelerometers with label identifying the quality of the activity the participant was doing. Testing data also comprised of accelerometer data without identifiable label(A-E).

The definitive instruction for this project is to use data to predict whether the exercise is being done properly or improperly based solely on accelerometer data measurements. The participants were instructed to perform the exercise either properly (Class A) or in a way which replicated 4 common weightlifting mistakes (Classes B, C, D, and E). 

**The question is, would we be able to predict appropriately each participants exercise manner by processing data gathered from classe(A-E) accelerometers? In that persuasion, we should apply some Machine Learning(ML) algorithms on 'trainData' and test them on given 'test dataset' for 'classe-level' based exercise manner prediction.**

### 1. Project write up Sequence: 
Here in drop down, I wrote the needed 'code' along with 'line-description' on each step of the process of ML-algorithms. I have used four machine learnig algorithms are Classification Tree, lda, gbm and random forest. I also used cross-validation with 'method' and 'k-folds' with number within all model. At the end of each ML-algorithm run, I presented the quantified 'accuracy rate'.

These findings would help us to analyse and predict the manner, in which participants did their exercise regime.

### 2. Data loading, visual overview and manipulation:

```{r, echo=TRUE}
# Necessary library loaded
library(easypackages)
suppressMessages(libraries("formattable", "dplyr", "tidyr", "ggplot2"))

# loading and reading data file from my desktop
trainDataSet <- read.csv("pml-training.csv", na.strings = c("", "NA"), header = TRUE)
testDataSet  <- read.csv("pml-testing.csv",  na.strings = c("", "NA"), header = TRUE)

# data dimension with row and columns
rbind ( trainDataSet = dim(trainDataSet), testDataSet = dim(testDataSet) )
```

### 2.a. Row-Columnar percentile presentation of classe(A-E) variables by each user

This columnar overview rendered in a 100% scale, which displays, how each user did their exercise regime(A-E), in what percentage of the total workout sequence.

```{r, echo=TRUE}  
# percentile projection of classe elements by user name
trainDataSet %>% count(classe, user_name) %>% group_by(user_name) %>% mutate(n=percent(n/sum(n),0))%>% spread(classe, n) %>% formattable(align = 'l') 
```

```{r, fig.width=8, fig.height=4, echo=TRUE}
ggplot(trainDataSet, aes(x=classe, fill=user_name)) + geom_bar() + xlab("Classe in bar segments") + ylab("User performances") + ggtitle("Sequence of 'classe-elements' by user")
```

**Plot analysis:** In this plots we can see that the all participants did 'Classe A' the most number of times and then slowly down to (B-E) pattern. They all started doing biceps curls the proper way (Class A), then proceeded with Class B, C to E. This plot gives us a percentile representation of each classe variable by each user which projects a visible exercise manner.

### 3. DataSet Partition and Exploratory data Cleaning:

```{r, echo=TRUE}
suppressMessages(library(caret))

# Create Data Partition with 0.75 is training and 0.25 test dataset
inTrain <- createDataPartition(trainDataSet$classe, p=0.75, list=FALSE)
TrainSet <- trainDataSet[inTrain, ]
TestSet  <- trainDataSet[-inTrain,]

# quick data-dimension after data partition
rbind ( TrainSet = dim(TrainSet), TestSet = dim(TestSet) )

#> **Note: some machine learning algoriths do not accept 'NA' values inside the DataSet.So we will do some 'NA' input manipulation.

# checking number of columns have 'NA' values with percentile projeciton in a table
table (NA_Value_Percent <- round(colMeans(is.na(TrainSet)), 2))
```

**Note:** We see that 100-variables have more than 98 percent data with "NA" input 'filled-in' and only 60-variables have complete data set. Variables with 98% data is 'NA' doesn't make any quantifiable effect in decision making anlytic processes.

```{r, echo=TRUE}
# so we'd eliminate all variable-columns, where more than 96% of the input are 'NA'
All_NA_columns <- sapply(TrainSet, function(x) mean(is.na(x))) > 0.96

# removing columns with 96% 'NA' only input from both 'Train and Test' dataset
TrainSet <- TrainSet[, All_NA_columns == FALSE]
TestSet  <- TestSet [, All_NA_columns == FALSE]

# a quick view of how many 'variable-columns' left after 'NA-elimination' process
rbind(TrainSet = dim(TrainSet), TestSet = dim(TestSet))
```

### 3.a. Covariates variation check
```{r, echo=TRUE}
# covariates variability check by setting 'saveMetrics = TRUE', return a data frame with predictor info
nzv <- nearZeroVar(TrainSet, saveMetrics = TRUE)
head(nzv)
```

**Analsis:** We see that most of the near-zero-variables(nzv) are 'false', so we don't need to eliminate any covariates.For further Simplification we will remove some unwarranted columns ('row-index' to 'not-relevant') from the dataset. 

```{r,echo=TRUE}
TrainSet <- TrainSet[, -(1:7)]
TestSet  <- TestSet [, -(1:7)]

# final dataSet dimension after all irrelevant column elimination
rbind ( TrainSet = dim(TrainSet), TestSet = dim(TestSet))
```

### 4. Machine Learning Algorithms with Cross Validation:

Here, I have used multiple Machine Learning algorithim in searching for high level model accuracy. I have used four algorithms Decision Tree, Linear Discriminant Analysis(lda), Gradient Boosting Method(gbm) and Random Forest(rf) to validate my search. Cross validation processes were included in 'trainControl' method with number of folds added. I used parallel-processing feature to reduce 'data-processing' time with 'gbm' and 'rf' model. I also used a confusion Matrix plot to visualize the level of accuracy of the classe variables with 'rf' model-algorithm only.

### Model.01: Decision (Classification) Tree

```{r, echo=TRUE}
# setting seed and loading library 'rattle' for decision tree 
suppressMessages(library(rattle));set.seed(666)

# designing the tree using 'rpart' method
control_dt <- trainControl(method="cv", number = 10)
model_Tree <- train(classe~., method = "rpart", data = TrainSet, trControl = control_dt)

# displaying 'model_Tree' node and leaf detail
print(model_Tree$finalModel, digits = 4)

# visualizing the decision tree with all detail 'leaf-palletes'
fancyRpartPlot(model_Tree$finalModel)

# running the 'rpart' model on 'TestSet' data and measure model accuracy rate
Test_pred <- predict(model_Tree, newdata = TestSet)
confusionMatrix(Test_pred, TestSet$classe)$overall['Accuracy']
```

**Upshot:** The accuracy rate with 'rpart' model on 'TestSet' data is 0.490, which is significantly lower and needs newer model exploration.

### Model.02: Linear Discriminant Analysis (lda)
```{r, echo=TRUE}
suppressMessages(library(MASS));set.seed(459)

# setting 'trainControl' feature for the 'lda' model with 8-fold cross-validation method
control_lda <- trainControl(method="cv", number = 10)
model_lda  <- train(classe~., trControl = control_lda, method="lda", data=TrainSet)

# using predict method to verify the model with 'TestSet' data and display model accuracy
lda_pred <- predict(model_lda, TestSet)
confusionMatrix(lda_pred, TestSet$classe)$overall['Accuracy']
```

**Upshot:** 'lda' model accuracy rate now rose up to at 0.70 on 'TestSet' data.

### Model.03: Gradient Boosting Method (gbm)
**Note:** 'gbm' and Random Forest(rf) models are computationally intensive, I have decided to use parallel processing to reduce computation timing. Parallel processing gave me a significant reduction(almost 60%, about 12 minutes) of time savings in ML-code processing.

```{r, echo=TRUE}
# all necessary library for 'gbm' model including (parallel and doParallel) for faster processing
suppressMessages(libraries("gbm", "plyr", "dplyr", "doParallel"));set.seed(9515)

# leaving a single core fo the operating system and registering the cluster
cluster <- makeCluster(detectCores() - 1)
registerDoParallel(cluster)

#> ** Note: 'trainControl' with repeated-cross-validation method, number specifies number of folds for k-fold cross-validation, and setting 'allowParallel= TRUE', mandates caret to use the cluster, we've register in previous steps.

control_gbm <- trainControl(method = "repeatedcv", number = 10, allowParallel = TRUE)
model_gbm <- train(classe~., preProcess= c("center", "scale"), trControl = control_gbm, method="gbm", data=TrainSet, verbose = FALSE)

# applying 'gbm' model on 'TestSet' data
gbm_pred <- predict(model_gbm, TestSet)

# confusion Matrix summary statistics with model 'accuracy' rate 
print(confusionMatrix(gbm_pred, TestSet$classe), digits = 4)

# confusionMatrix(gbm_pred, TestSet$classe)$StatisticsbyClass
confusionMatrix(gbm_pred, TestSet$classe)$overall['Accuracy']
```
**Upshot:** There is a considerable accuracy rate increase  up to (0.963) compare to 'lda' model (0.701).

### Model.04: Random Forest (rf)

```{r, echo=TRUE}
# loading library, setting seed and 'registering-parallel-processing'
suppressMessages(library(randomForest));set.seed(969)
registerDoParallel(cluster)

# setting control feature with method 'repeatedcv' and adding parallel processing cluster
Control_Rfo <- trainControl(method = "repeatedcv", number = 9, allowParallel = TRUE)

# running 'rf' model with proprocessing method and predefined control feature
model_Rfo  <- train(classe~., method = "rf", preProcess=c("center", "scale"),  data=TrainSet, trControl = Control_Rfo, verboseIter =FALSE)

# Evaluating the model on 'TestSet' data and calculating confusionMatrix
Rfo_pred <- predict(model_Rfo, TestSet)
confusion_Rfo <- confusionMatrix(Rfo_pred, TestSet$classe)

# confusion Matrix summary statistics with 'accuracy' rate
print(confusionMatrix(Rfo_pred, TestSet$classe), digits = 4 )
confusion_Rfo$overall['Accuracy']
```

```{r, fig.width=8, fig.height=4, echo=TRUE}
if(FALSE){
# ploting the 'confusion Matrix' of "Random Forest" model for classe-steps verification
plot.03 <- plot(confusion_Rfo$table, col = confusion_Rfo$byClass, main = paste("Random Forest Model Accuracy =",
            round(confusion_Rfo$overall['Accuracy'], 4)))
}
```

**Upshot:** Random forest model by far is predicting the best 'accuracy rate' 0.9955 with least  'out-of-sample error' is 0.004 rate.

### Out-Of-Sample error calculation: 

**Random Forest Model**     out of sample error:(1 - 0.9955139) = **0.005**

**Gradient Boosting Model**         out of sample error:(1- 0.9665579) = **0.040**

**Linear Discriminant Analysis**              out of sample error:(1- 0.694739) = 0.305

**Classification or Decision tree**               out of sample  error:(1- 0.4912316) = 0.508

*Note:* Every single time running these algorithms produces slightly different accuracy rates and tree pallets. 

***

### Applying ML-models on 20 test-case data set: 
Applying only three machine learning('rf','gbm','lda') algorithm model on to the 20 test-cases ('testDataSet') dataset, provided with the project instruction for level-based prediction.

```{r, echo=TRUE}
print(predict(model_Rfo, newdata = testDataSet))
print(predict(model_gbm, newdata = testDataSet))
```

**Analysis:**  Remarkably 'random-forest' and 'gbm' model both made exact same 'level' of prediction on 'testDataSet', which proves high level of accuracy proximity.

```{r, echo=TRUE}
print(predict(model_lda, newdata = testDataSet))

# finally folding the parallel-processing cluster 
stopCluster(cluster)
# forcing 'R' to return single threading process
registerDoSEQ()
```
